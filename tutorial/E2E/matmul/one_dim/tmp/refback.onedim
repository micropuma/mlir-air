module {
  llvm.mlir.global internal constant @__airrt_string_matadd_0("matadd_0") {addr_space = 0 : i32}
  func.func private @__airrt_segment_load(!llvm.ptr) -> i64 attributes {llvm.emit_c_interface}
  llvm.mlir.global internal constant @__airrt_string_herd_0("herd_0") {addr_space = 0 : i32}
  func.func private @__airrt_herd_load(!llvm.ptr) -> i64 attributes {llvm.emit_c_interface}
  func.func @matadd(%arg0: memref<2x2xf32>, %arg1: memref<2x2xf32>, %arg2: memref<2x2xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.addressof @__airrt_string_herd_0 : !llvm.ptr
    %1 = llvm.mlir.addressof @__airrt_string_matadd_0 : !llvm.ptr
    %2 = llvm.mlir.constant(1 : i32) : i32
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %c0_i64 = arith.constant 0 : i64
    %c6_i32 = arith.constant 6 : i32
    %c5_i32 = arith.constant 5 : i32
    %c4_i32 = arith.constant 4 : i32
    %alloc = memref.alloc() : memref<2x2xf32>
    affine.for %arg3 = 0 to 2 {
      affine.for %arg4 = 0 to 2 {
        %3 = arith.index_cast %arg3 : index to i64
        %4 = arith.index_cast %arg4 : index to i64
        %5 = llvm.alloca %2 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
        %cast = memref.cast %arg0 : memref<2x2xf32> to memref<?x?xf32>
        func.call @__airrt_dma_nd_memcpy_2d0f32(%5, %c4_i32, %3, %4, %cast, %c0_i64, %c0_i64, %3, %4, %c1_i64, %c1_i64, %c1_i64, %c1_i64, %c0_i64, %c0_i64, %c2_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
      }
    } {affine_opt_label = "tiling"}
    affine.for %arg3 = 0 to 2 {
      affine.for %arg4 = 0 to 2 {
        %3 = arith.index_cast %arg3 : index to i64
        %4 = arith.index_cast %arg4 : index to i64
        %5 = llvm.alloca %2 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
        %cast = memref.cast %arg1 : memref<2x2xf32> to memref<?x?xf32>
        func.call @__airrt_dma_nd_memcpy_2d0f32(%5, %c5_i32, %3, %4, %cast, %c0_i64, %c0_i64, %3, %4, %c1_i64, %c1_i64, %c1_i64, %c1_i64, %c0_i64, %c0_i64, %c2_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
      }
    } {affine_opt_label = "tiling"}
    affine.for %arg3 = 0 to 2 {
      affine.for %arg4 = 0 to 2 {
        %3 = arith.index_cast %arg3 : index to i64
        %4 = arith.index_cast %arg4 : index to i64
        %5 = llvm.alloca %2 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
        %cast = memref.cast %alloc : memref<2x2xf32> to memref<?x?xf32>
        func.call @__airrt_dma_nd_memcpy_2d0f32(%5, %c6_i32, %3, %4, %cast, %c0_i64, %c0_i64, %3, %4, %c1_i64, %c1_i64, %c1_i64, %c1_i64, %c0_i64, %c0_i64, %c2_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
      }
    } {affine_opt_label = "tiling"}
    affine.for %arg3 = 0 to 1 {
      affine.for %arg4 = 0 to 1 {
        %3 = func.call @__airrt_segment_load(%1) : (!llvm.ptr) -> i64
        affine.for %arg5 = 0 to 1 {
          %4 = func.call @__airrt_herd_load(%0) : (!llvm.ptr) -> i64
        }
      }
    } {affine_opt_label = "tiling"}
    memref.copy %alloc, %arg2 : memref<2x2xf32> to memref<2x2xf32>
    memref.dealloc %alloc : memref<2x2xf32>
    return
  }
  func.func private @__airrt_dma_nd_memcpy_2d0f32(!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) attributes {llvm.emit_c_interface}
}
