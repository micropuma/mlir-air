module {
  llvm.mlir.global internal constant @__airrt_string_matadd_0("matadd_0") {addr_space = 0 : i32}
  func.func private @__airrt_segment_load(!llvm.ptr) -> i64 attributes {llvm.emit_c_interface}
  llvm.mlir.global internal constant @__airrt_string_herd_0("herd_0") {addr_space = 0 : i32}
  func.func private @__airrt_herd_load(!llvm.ptr) -> i64 attributes {llvm.emit_c_interface}
  func.func @matadd(%arg0: memref<2x2xf32>, %arg1: memref<2x2xf32>, %arg2: memref<2x2xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i32) : i32
    %1 = llvm.mlir.addressof @__airrt_string_herd_0 : !llvm.ptr
    %2 = llvm.mlir.addressof @__airrt_string_matadd_0 : !llvm.ptr
    %c2_i64 = arith.constant 2 : i64
    %c3_i32 = arith.constant 3 : i32
    %c2_i32 = arith.constant 2 : i32
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i32 = arith.constant 1 : i32
    %c2 = arith.constant 2 : index
    %alloc = memref.alloc() : memref<2x2xf32>
    affine.for %arg3 = 0 to 1 {
      affine.for %arg4 = 0 to 1 {
        %3 = func.call @__airrt_segment_load(%2) : (!llvm.ptr) -> i64
        affine.for %arg5 = 0 to 1 {
          %4 = func.call @__airrt_herd_load(%1) : (!llvm.ptr) -> i64
          affine.for %arg6 = 0 to 1 {
            affine.for %arg7 = 0 to 4 {
              %5 = arith.remsi %arg7, %c2 : index
              %6 = arith.divsi %arg7, %c2 : index
              %7 = arith.index_cast %arg7 : index to i64
              %8 = arith.index_cast %arg6 : index to i64
              %9 = arith.index_cast %6 : index to i64
              %10 = arith.index_cast %5 : index to i64
              %11 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
              %cast = memref.cast %arg0 : memref<2x2xf32> to memref<?x?xf32>
              func.call @__airrt_dma_nd_memcpy_2d0f32(%11, %c1_i32, %7, %8, %cast, %c0_i64, %c0_i64, %9, %10, %c1_i64, %c1_i64, %c1_i64, %c1_i64, %c0_i64, %c0_i64, %c2_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
              %12 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
              %cast_0 = memref.cast %arg1 : memref<2x2xf32> to memref<?x?xf32>
              func.call @__airrt_dma_nd_memcpy_2d0f32(%12, %c2_i32, %7, %8, %cast_0, %c0_i64, %c0_i64, %9, %10, %c1_i64, %c1_i64, %c1_i64, %c1_i64, %c0_i64, %c0_i64, %c2_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
              func.call @__airrt_wait_all_0_2(%12, %11) : (!llvm.ptr, !llvm.ptr) -> ()
              %13 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
              %cast_1 = memref.cast %alloc : memref<2x2xf32> to memref<?x?xf32>
              func.call @__airrt_dma_nd_memcpy_2d0f32(%13, %c3_i32, %7, %8, %cast_1, %c0_i64, %c0_i64, %9, %10, %c1_i64, %c1_i64, %c1_i64, %c1_i64, %c0_i64, %c0_i64, %c2_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
              func.call @__airrt_wait_all_0_1(%13) : (!llvm.ptr) -> ()
            } {air.herd = "inner"}
          } {air.herd = "outer"}
        }
      }
    } {affine_opt_label = "tiling"}
    memref.copy %alloc, %arg2 : memref<2x2xf32> to memref<2x2xf32>
    memref.dealloc %alloc : memref<2x2xf32>
    return
  }
  func.func private @__airrt_dma_nd_memcpy_2d0f32(!llvm.ptr, i32, i64, i64, memref<?x?xf32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) attributes {llvm.emit_c_interface}
  func.func private @__airrt_wait_all_0_2(!llvm.ptr, !llvm.ptr) attributes {llvm.emit_c_interface}
  func.func private @__airrt_wait_all_0_1(!llvm.ptr) attributes {llvm.emit_c_interface}
}
