module {
  llvm.mlir.global internal constant @__airrt_string_segment_0("segment_0") {addr_space = 0 : i32}
  func.func private @__airrt_segment_load(!llvm.ptr) -> i64 attributes {llvm.emit_c_interface}
  llvm.mlir.global internal constant @__airrt_string_herd_0("herd_0") {addr_space = 0 : i32}
  func.func private @__airrt_herd_load(!llvm.ptr) -> i64 attributes {llvm.emit_c_interface}
  func.func @forward(%arg0: memref<64x64xi32>, %arg1: memref<64x64xi32>, %arg2: memref<64x64xi32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(1 : i32) : i32
    %1 = llvm.mlir.addressof @__airrt_string_herd_0 : !llvm.ptr
    %2 = llvm.mlir.addressof @__airrt_string_segment_0 : !llvm.ptr
    %c32_i64 = arith.constant 32 : i64
    %c64_i64 = arith.constant 64 : i64
    %c5_i32 = arith.constant 5 : i32
    %c4_i32 = arith.constant 4 : i32
    %c3_i32 = arith.constant 3 : i32
    %c2_i32 = arith.constant 2 : i32
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i32 = arith.constant 1 : i32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %alloc = memref.alloc() : memref<64x64xi32>
    affine.for %arg3 = 0 to 1 {
      affine.for %arg4 = 0 to 1 {
        %3 = func.call @__airrt_segment_load(%2) : (!llvm.ptr) -> i64
        affine.for %arg5 = 0 to 1 {
          %4 = func.call @__airrt_herd_load(%1) : (!llvm.ptr) -> i64
          affine.for %arg6 = 0 to 1 {
            affine.for %arg7 = 0 to 4 {
              %5 = arith.remsi %arg7, %c2 : index
              %6 = arith.divsi %arg7, %c2 : index
              %7 = arith.muli %6, %c32 : index
              %8 = arith.muli %5, %c32 : index
              %9 = func.call @__airrt_wait_all_1_0() : () -> !llvm.ptr
              %10 = scf.for %arg8 = %c0 to %c64 step %c32 iter_args(%arg9 = %9) -> (!llvm.ptr) {
                %17 = arith.cmpi eq, %6, %c0 : index
                %18 = arith.cmpi sge, %5, %c0 : index
                %19 = arith.andi %17, %18 : i1
                %20 = arith.subi %c1, %5 : index
                %21 = arith.cmpi sge, %20, %c0 : index
                %22 = arith.andi %19, %21 : i1
                %23 = scf.if %22 -> (!llvm.ptr) {
                  %32 = func.call @__airrt_wait_all_1_1(%arg9) : (!llvm.ptr) -> !llvm.ptr
                  %33 = arith.index_cast %arg7 : index to i64
                  %34 = arith.index_cast %arg6 : index to i64
                  %35 = arith.index_cast %arg8 : index to i64
                  %36 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
                  %cast_0 = memref.cast %arg0 : memref<64x64xi32> to memref<?x?xi32>
                  func.call @__airrt_dma_nd_memcpy_2d0i32(%36, %c1_i32, %33, %34, %cast_0, %c0_i64, %c0_i64, %c0_i64, %35, %c1_i64, %c1_i64, %c32_i64, %c32_i64, %c0_i64, %c0_i64, %c64_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xi32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
                  scf.yield %36 : !llvm.ptr
                } else {
                  %32 = func.call @__airrt_wait_all_1_1(%arg9) : (!llvm.ptr) -> !llvm.ptr
                  %33 = arith.index_cast %arg7 : index to i64
                  %34 = arith.index_cast %arg6 : index to i64
                  %35 = arith.index_cast %arg8 : index to i64
                  %36 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
                  %cast_0 = memref.cast %arg0 : memref<64x64xi32> to memref<?x?xi32>
                  func.call @__airrt_dma_nd_memcpy_2d0i32(%36, %c2_i32, %33, %34, %cast_0, %c0_i64, %c0_i64, %c32_i64, %35, %c1_i64, %c1_i64, %c32_i64, %c32_i64, %c0_i64, %c0_i64, %c64_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xi32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
                  scf.yield %36 : !llvm.ptr
                }
                %24 = arith.cmpi sge, %6, %c0 : index
                %25 = arith.subi %c1, %6 : index
                %26 = arith.cmpi sge, %25, %c0 : index
                %27 = arith.andi %24, %26 : i1
                %28 = arith.cmpi eq, %5, %c0 : index
                %29 = arith.andi %27, %28 : i1
                %30 = scf.if %29 -> (!llvm.ptr) {
                  %32 = func.call @__airrt_wait_all_1_1(%arg9) : (!llvm.ptr) -> !llvm.ptr
                  %33 = arith.index_cast %arg7 : index to i64
                  %34 = arith.index_cast %arg6 : index to i64
                  %35 = arith.index_cast %arg8 : index to i64
                  %36 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
                  %cast_0 = memref.cast %arg1 : memref<64x64xi32> to memref<?x?xi32>
                  func.call @__airrt_dma_nd_memcpy_2d0i32(%36, %c3_i32, %33, %34, %cast_0, %c0_i64, %c0_i64, %35, %c0_i64, %c1_i64, %c1_i64, %c32_i64, %c32_i64, %c0_i64, %c0_i64, %c64_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xi32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
                  scf.yield %36 : !llvm.ptr
                } else {
                  %32 = func.call @__airrt_wait_all_1_1(%arg9) : (!llvm.ptr) -> !llvm.ptr
                  %33 = arith.index_cast %arg7 : index to i64
                  %34 = arith.index_cast %arg6 : index to i64
                  %35 = arith.index_cast %arg8 : index to i64
                  %36 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
                  %cast_0 = memref.cast %arg1 : memref<64x64xi32> to memref<?x?xi32>
                  func.call @__airrt_dma_nd_memcpy_2d0i32(%36, %c4_i32, %33, %34, %cast_0, %c0_i64, %c0_i64, %35, %c32_i64, %c1_i64, %c1_i64, %c32_i64, %c32_i64, %c0_i64, %c0_i64, %c64_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xi32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
                  scf.yield %36 : !llvm.ptr
                }
                func.call @__airrt_wait_all_0_2(%30, %23) : (!llvm.ptr, !llvm.ptr) -> ()
                %31 = func.call @__airrt_wait_all_1_0() : () -> !llvm.ptr
                scf.yield %31 : !llvm.ptr
              }
              %11 = func.call @__airrt_wait_all_1_1(%10) : (!llvm.ptr) -> !llvm.ptr
              %12 = arith.index_cast %arg7 : index to i64
              %13 = arith.index_cast %arg6 : index to i64
              %14 = arith.index_cast %7 : index to i64
              %15 = arith.index_cast %8 : index to i64
              %16 = llvm.alloca %0 x i32 {alignment = 4 : i64} : (i32) -> !llvm.ptr
              %cast = memref.cast %alloc : memref<64x64xi32> to memref<?x?xi32>
              func.call @__airrt_dma_nd_memcpy_2d0i32(%16, %c5_i32, %12, %13, %cast, %c0_i64, %c0_i64, %14, %15, %c1_i64, %c1_i64, %c32_i64, %c32_i64, %c0_i64, %c0_i64, %c64_i64) : (!llvm.ptr, i32, i64, i64, memref<?x?xi32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> ()
              func.call @__airrt_wait_all_0_1(%16) : (!llvm.ptr) -> ()
            } {air.herd = "inner"}
          } {air.herd = "outer"}
        }
      }
    } {affine_opt_label = "tiling"}
    memref.copy %alloc, %arg2 : memref<64x64xi32> to memref<64x64xi32>
    return
  }
  func.func private @__airrt_wait_all_1_0() -> !llvm.ptr attributes {llvm.emit_c_interface}
  func.func private @__airrt_wait_all_1_1(!llvm.ptr) -> !llvm.ptr attributes {llvm.emit_c_interface}
  func.func private @__airrt_dma_nd_memcpy_2d0i32(!llvm.ptr, i32, i64, i64, memref<?x?xi32>, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) attributes {llvm.emit_c_interface}
  func.func private @__airrt_wait_all_0_2(!llvm.ptr, !llvm.ptr) attributes {llvm.emit_c_interface}
  func.func private @__airrt_wait_all_0_1(!llvm.ptr) attributes {llvm.emit_c_interface}
}
