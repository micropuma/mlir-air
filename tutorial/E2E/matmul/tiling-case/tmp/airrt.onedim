#set = affine_set<()[s0, s1] : (s0 == 0, s1 >= 0, -s1 + 1 >= 0)>
#set1 = affine_set<()[s0, s1] : (s0 - 1 == 0, s1 >= 0, -s1 + 1 >= 0)>
#set2 = affine_set<()[s0, s1] : (s0 >= 0, -s0 + 1 >= 0, s1 == 0)>
#set3 = affine_set<()[s0, s1] : (s0 >= 0, -s0 + 1 >= 0, s1 - 1 == 0)>
module {
  airrt.module_metadata{
    airrt.segment_metadata attributes {dma_allocations = [], sym_name = "segment_0"}{
      airrt.herd_metadata {dma_allocations = [{channel = 0 : i64, col = 0 : i64, id = 5 : i64, location = 7 : i64, row = 3 : i64}, {channel = 1 : i64, col = 0 : i64, id = 5 : i64, location = 7 : i64, row = 2 : i64}, {channel = 0 : i64, col = 0 : i64, id = 5 : i64, location = 10 : i64, row = 1 : i64}, {channel = 1 : i64, col = 0 : i64, id = 5 : i64, location = 10 : i64, row = 0 : i64}, {channel = 2 : i64, col = 0 : i64, id = 2 : i64, location = 7 : i64, row = 3 : i64}, {channel = 3 : i64, col = 0 : i64, id = 4 : i64, location = 7 : i64, row = 3 : i64}, {channel = 2 : i64, col = 0 : i64, id = 2 : i64, location = 10 : i64, row = 2 : i64}, {channel = 3 : i64, col = 0 : i64, id = 3 : i64, location = 10 : i64, row = 2 : i64}, {channel = 2 : i64, col = 0 : i64, id = 1 : i64, location = 11 : i64, row = 1 : i64}, {channel = 3 : i64, col = 0 : i64, id = 4 : i64, location = 11 : i64, row = 1 : i64}, {channel = 2 : i64, col = 0 : i64, id = 1 : i64, location = 18 : i64, row = 0 : i64}, {channel = 3 : i64, col = 0 : i64, id = 3 : i64, location = 18 : i64, row = 0 : i64}], sym_name = "herd_0"}
    }
  }
  func.func private @linalg_fill_i32_view32x32xi32as2(i32, memref<32x32xi32, 2>) attributes {link_with = "mm.o", llvm.emit_c_interface}
  func.func private @linalg_matmul_view32x32xi32as2_view32x32xi32as2_view32x32xi32as2(memref<32x32xi32, 2>, memref<32x32xi32, 2>, memref<32x32xi32, 2>) attributes {link_with = "mm.o", llvm.emit_c_interface}
  func.func @forward(%arg0: memref<64x64xi32>, %arg1: memref<64x64xi32>, %arg2: memref<64x64xi32>) {
    %c32_i64 = arith.constant 32 : i64
    %c64_i64 = arith.constant 64 : i64
    %c5_i32 = arith.constant 5 : i32
    %c4_i32 = arith.constant 4 : i32
    %c3_i32 = arith.constant 3 : i32
    %c2_i32 = arith.constant 2 : i32
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i32 = arith.constant 1 : i32
    %c2 = arith.constant 2 : index
    %c0_i32 = arith.constant 0 : i32
    %c64 = arith.constant 64 : index
    %c32 = arith.constant 32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %alloc = memref.alloc() : memref<64x64xi32>
    affine.for %arg3 = 0 to 1 {
      affine.for %arg4 = 0 to 1 {
        %p = airrt.segment_load "segment_0" : i64
        affine.for %arg5 = 0 to 1 {
          %h = airrt.herd_load "herd_0" () : () -> i64
          affine.for %arg6 = 0 to 1 {
            affine.for %arg7 = 0 to 4 {
              %0 = arith.remsi %arg7, %c2 : index
              %1 = arith.divsi %arg7, %c2 : index
              %2 = arith.muli %1, %c32 : index
              %3 = arith.muli %0, %c32 : index
              %alloc_0 = memref.alloc() : memref<32x32xi32, 2>
              func.call @linalg_fill_i32_view32x32xi32as2(%c0_i32, %alloc_0) : (i32, memref<32x32xi32, 2>) -> ()
              %4 = airrt.wait_all : !airrt.event
              %5 = scf.for %arg8 = %c0 to %c64 step %c32 iter_args(%arg9 = %4) -> (!airrt.event) {
                %alloc_1 = memref.alloc() : memref<32x32xi32, 2>
                %alloc_2 = memref.alloc() : memref<32x32xi32, 2>
                %12 = arith.cmpi eq, %1, %c0 : index
                %13 = arith.cmpi sge, %0, %c0 : index
                %14 = arith.andi %12, %13 : i1
                %15 = arith.subi %c1, %0 : index
                %16 = arith.cmpi sge, %15, %c0 : index
                %17 = arith.andi %14, %16 : i1
                %18 = scf.if %17 -> (!airrt.event) {
                  %27 = airrt.wait_all %arg9 : !airrt.event
                  %28 = arith.index_cast %arg7 : index to i64
                  %29 = arith.index_cast %arg6 : index to i64
                  %30 = arith.index_cast %arg8 : index to i64
                  %31 = airrt.dma_memcpy_nd(%c1_i32, %28, %29, %arg0[%c0_i64, %c0_i64, %c0_i64, %30], [%c1_i64, %c1_i64, %c32_i64, %c32_i64], [%c0_i64, %c0_i64, %c64_i64]) {broadcast_set = #set} : (i32, i64, i64, memref<64x64xi32>, [i64, i64, i64, i64], [i64, i64, i64, i64], [i64, i64, i64]) : !airrt.event
                  scf.yield %31 : !airrt.event
                } else {
                  %27 = airrt.wait_all %arg9 : !airrt.event
                  %28 = arith.index_cast %arg7 : index to i64
                  %29 = arith.index_cast %arg6 : index to i64
                  %30 = arith.index_cast %arg8 : index to i64
                  %31 = airrt.dma_memcpy_nd(%c2_i32, %28, %29, %arg0[%c0_i64, %c0_i64, %c32_i64, %30], [%c1_i64, %c1_i64, %c32_i64, %c32_i64], [%c0_i64, %c0_i64, %c64_i64]) {broadcast_set = #set1} : (i32, i64, i64, memref<64x64xi32>, [i64, i64, i64, i64], [i64, i64, i64, i64], [i64, i64, i64]) : !airrt.event
                  scf.yield %31 : !airrt.event
                }
                %19 = arith.cmpi sge, %1, %c0 : index
                %20 = arith.subi %c1, %1 : index
                %21 = arith.cmpi sge, %20, %c0 : index
                %22 = arith.andi %19, %21 : i1
                %23 = arith.cmpi eq, %0, %c0 : index
                %24 = arith.andi %22, %23 : i1
                %25 = scf.if %24 -> (!airrt.event) {
                  %27 = airrt.wait_all %arg9 : !airrt.event
                  %28 = arith.index_cast %arg7 : index to i64
                  %29 = arith.index_cast %arg6 : index to i64
                  %30 = arith.index_cast %arg8 : index to i64
                  %31 = airrt.dma_memcpy_nd(%c3_i32, %28, %29, %arg1[%c0_i64, %c0_i64, %30, %c0_i64], [%c1_i64, %c1_i64, %c32_i64, %c32_i64], [%c0_i64, %c0_i64, %c64_i64]) {broadcast_set = #set2} : (i32, i64, i64, memref<64x64xi32>, [i64, i64, i64, i64], [i64, i64, i64, i64], [i64, i64, i64]) : !airrt.event
                  scf.yield %31 : !airrt.event
                } else {
                  %27 = airrt.wait_all %arg9 : !airrt.event
                  %28 = arith.index_cast %arg7 : index to i64
                  %29 = arith.index_cast %arg6 : index to i64
                  %30 = arith.index_cast %arg8 : index to i64
                  %31 = airrt.dma_memcpy_nd(%c4_i32, %28, %29, %arg1[%c0_i64, %c0_i64, %30, %c32_i64], [%c1_i64, %c1_i64, %c32_i64, %c32_i64], [%c0_i64, %c0_i64, %c64_i64]) {broadcast_set = #set3} : (i32, i64, i64, memref<64x64xi32>, [i64, i64, i64, i64], [i64, i64, i64, i64], [i64, i64, i64]) : !airrt.event
                  scf.yield %31 : !airrt.event
                }
                airrt.wait_all %25, %18
                func.call @linalg_matmul_view32x32xi32as2_view32x32xi32as2_view32x32xi32as2(%alloc_1, %alloc_2, %alloc_0) : (memref<32x32xi32, 2>, memref<32x32xi32, 2>, memref<32x32xi32, 2>) -> ()
                %26 = airrt.wait_all : !airrt.event
                memref.dealloc %alloc_1 : memref<32x32xi32, 2>
                memref.dealloc %alloc_2 : memref<32x32xi32, 2>
                scf.yield %26 : !airrt.event
              }
              %6 = airrt.wait_all %5 : !airrt.event
              %7 = arith.index_cast %arg7 : index to i64
              %8 = arith.index_cast %arg6 : index to i64
              %9 = arith.index_cast %2 : index to i64
              %10 = arith.index_cast %3 : index to i64
              %11 = airrt.dma_memcpy_nd(%c5_i32, %7, %8, %alloc[%c0_i64, %c0_i64, %9, %10], [%c1_i64, %c1_i64, %c32_i64, %c32_i64], [%c0_i64, %c0_i64, %c64_i64]) : (i32, i64, i64, memref<64x64xi32>, [i64, i64, i64, i64], [i64, i64, i64, i64], [i64, i64, i64]) : !airrt.event
              airrt.wait_all %11
              memref.dealloc %alloc_0 : memref<32x32xi32, 2>
            } {air.herd = "inner"}
          } {air.herd = "outer"}
        }
      }
    } {affine_opt_label = "tiling"}
    memref.copy %alloc, %arg2 : memref<64x64xi32> to memref<64x64xi32>
    return
  }
}
